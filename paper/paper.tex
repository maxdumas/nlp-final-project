%%%%%%%% BEGIN PREAMBLE %%%%%%%%

\documentclass[11pt]{article}
\usepackage{acl2015, latexsym, url, times, amssymb, amsmath, amsfonts, mathtools}

\DeclareMathOperator*{\argmax}{argmax}

%%%%%%%% END PREAMBLE %%%%%%%%
\title{Natural Language Processing\\
Fall 2015 Final Project:\\
Sense Disambiguation with the\\
Naive Bayes Classifier}
\author{Maximillian Dumas}


\begin{document}
\maketitle

\begin{abstract}
This paper examines an attempt to implement the Naive Bayes Classifier algorithm for the purpose of performing the task of Word Sense Disambiguation. In it we will discuss the problem of Word Sense Disambiguation broadly, along with a thorough explanation of the Naive Bayes algorithm. Finally, we will present an implementation of the algorithm in Python and analyze the results of the application of the alogrithm and difficulties that arose in its application.
\end{abstract}

\section{Introduction}
An incredibly important aspect of computational understanding of human language is word meaning. Without word meaning, effective interpretation of sentences and texts at large is impossible. The meaning of a word occurrence most concisely is the triple of its part-of-speech, lemma form. We call each unique triple that can be assigned to a word form a \emph{sense} of that word. The information provided to us by identifying the sense of a word is critical in any natural language processing task that requires identifying the precise usage of a word. Examples of tasks where this is the case include text classification, question answering, text summarization, and information extraction.

Knowledge of word sense is thus an essential element in natural language processing, but determining word senses by hand is often impractical. Humans are exceptionally good at determining the senses of words in context, and our language is built around that. However, this makes the task of word sense disambiguation difficult for computers, as our language is full of homonyms and homographs, and many words are often used with specific senses in very subtle ways. Yet computers must be able to parse and understand our words if we are to empower them with our language, and we cannot annotate all texts manually. Thus enters the field of word sense disambiguation, which aims to provide computers with the tools to automatically determine the sense of words in a given context. The desired end result of a word sense disambiguation task is that all words in the input have been assigned a single, correct sense that correctly reflects the text's real meaning.

There are many approaches to word sense disambiguation---in particular, machine learning tasks can be broadly adapted to solve the problem. One of the simpler and more efficient of such methods is the Naive Bayes Classifier, which is a probabilistic method that manages to achieve good results while still remaining fairly simple in implementation.

\section{The Naive Bayes Classifier}
As mentioned previously, humans are highly proficient at identifying the sense of a word in use when provided the context of that word. We identify contextual cues that tell us what a specific word's intended meaning must be. We will call these cues \emph{features}, and they may include anything that allows a reader to narrow down the choices for possible meanings. Two of the most important kinds of features are those that provide local context and those that provide global context. Local context refers to looking at how the word sits in the sentence, namely what words it is neighboring, their parts of speech, etc. Global context is the larger semantic space that the word exists in, like the topic of the sentence, paragraph, or text as whole. Every word in a given corpus can be assigned a vector of features providing this context, which we will denote $F$.

Given a set $F$ for each word $w$, we can then view the problem of disambiguating word sense from a probabilistic perspective. For all $w$, we are then attempting to choose the sense that is maximally probable, $s^*$, given $w$ and the observed feature set $F(s)$ for all words with sense $s$. That is:

\begin{equation} \label{eq:fundamental}
\begin{split}
s^*(w) &= \argmax_{s \in S(w)} P(s|F(s), w) \\
       &= \argmax_{s \in S(w)} \frac{P(F(s)|s,w)P(s|w)}{P(F(s))}
\end{split}
\end{equation}

This equation is simple and conceptually effective. But in practice, even the second form is difficult to use---to compute the probability that a given feature vector will occur requires that that entire feature vector have been encountered for that sense before. For large feature vectors the likelihood of this is extremely low, and requires prohibitively large sets of training data. The Naive Bayes Classifier (NBC) is an attempt to minimize the combinatorics of the features by making the \emph{na\"ive} assumption that the occurrence of each feature is an independent event. With this assumption, we may say that ${P(F(s)|s) = \prod_{f \in F(s)} P(f|s)}$. This assumption is certainly not always true, and is what gives the NBC its na\"ive qualifier. Indeed this equation is often an approximation at best, but usually it is sufficiently accurate to be useable.

Substituting this value in and removing $P(F(s))$, as it is constant for all $s$ and will not affect which is chosen for any given word, we arrive at the Naive Bayes Classifier equation:

\begin{equation} \label{eq:nbc}
s^*(w) = \argmax_{s \in S(w)} P(s|w)\!\!\prod_{f \in F(s)}\!\!P(f|s)
\end{equation}

With this formulation, the primary problem in effectively disambiguating word senses is one of training. Identifying the features that accurately determine word sense and finding sense-tagged corpora that exhibit those features are the primary problems in applying the NBC.

\section{Implementation}
Discuss exactly what techniques were used to implement the algorithm. What technical issues/limitations resulted in changes? Figure out exactly what techniques we're co-opting for pos-tagging, lemmatizing, etc!

Talk about the baseline also.

Switched to POS-informed lemmatizer instead of simply choosing lemma with largest count, doubled accuracy.

\section{Analysis}
Discuss here the performance and results of applying the algorithm.

Discuss here the difficulty in applying features successfully.

\section{Conclusion}
Discuss here what went wrong, what went right, avenues for improvement, further reading, etc.

Discuss here how it is difficult to analyze the decisions that the Naive Bayes Classifier makes and why.

\end{document}